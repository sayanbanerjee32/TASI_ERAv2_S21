{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMIvfFhjh20//pXqRDiQ0Kt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e0ad1c8ef2f4a01b819caa19e001f5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7c917c376903419f9bff0545b9fd4634",
              "IPY_MODEL_498ccbaf880d4ef6b24a2e035718b320",
              "IPY_MODEL_42fcae08154f4b3998a97d0d11f791a1"
            ],
            "layout": "IPY_MODEL_42af15fb5d1649ccb80c0ace7de3a027"
          }
        },
        "7c917c376903419f9bff0545b9fd4634": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8307072a74394924ac8c4fa306e3d3af",
            "placeholder": "​",
            "style": "IPY_MODEL_27ba76910dc549438dc48be30b6fae67",
            "value": "ckpt.pt: 100%"
          }
        },
        "498ccbaf880d4ef6b24a2e035718b320": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08b34f61482c440ebc0f5ff376249284",
            "max": 1544198298,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6bbdddbf83ff48628f45dac89a85da4a",
            "value": 1544198298
          }
        },
        "42fcae08154f4b3998a97d0d11f791a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b2a63244394477daab48887f6e11297",
            "placeholder": "​",
            "style": "IPY_MODEL_77f7e31c535a4aa58aacb5133f347b63",
            "value": " 1.54G/1.54G [01:16&lt;00:00, 16.7MB/s]"
          }
        },
        "42af15fb5d1649ccb80c0ace7de3a027": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8307072a74394924ac8c4fa306e3d3af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27ba76910dc549438dc48be30b6fae67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08b34f61482c440ebc0f5ff376249284": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6bbdddbf83ff48628f45dac89a85da4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8b2a63244394477daab48887f6e11297": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77f7e31c535a4aa58aacb5133f347b63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/TASI_ERAv2_S21/blob/main/gpt2_training_cusom_input.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tiktoken\n",
        "!pip install -Uq pynvml"
      ],
      "metadata": {
        "id": "bzpHvONH-2g3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assignment repo\n",
        "!git clone https://github.com/sayanbanerjee32/TASI_ERAv2_S21.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvO9-EU2GXo0",
        "outputId": "9497db95-4dac-439a-a526-0e07cfa1aa4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TASI_ERAv2_S21'...\n",
            "remote: Enumerating objects: 35, done.\u001b[K\n",
            "remote: Counting objects: 100% (35/35), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "Receiving objects: 100% (35/35), 294.87 KiB | 9.21 MiB/s, done.\n",
            "remote: Total 35 (delta 12), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Resolving deltas: 100% (12/12), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## move python files to outside\n",
        "!mv TASI_ERAv2_S21/*.py ."
      ],
      "metadata": {
        "id": "gHEo9KKoGczf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model_gpt2 import GPTConfig, GPT\n",
        "from data_loader_lite import DataLoaderLite"
      ],
      "metadata": {
        "id": "VXdyZHDrsTy2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "GEiP8kqAHKrw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on custom input data"
      ],
      "metadata": {
        "id": "6C0WmV7i-JYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### hypre params\n",
        "max_lr = 6e-4\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "warmup_steps = 50\n",
        "max_steps = 1000\n",
        "\n",
        "# save / log config\n",
        "out_dir = 'saved_model'\n",
        "save_interval = 100\n",
        "log_interval = 50"
      ],
      "metadata": {
        "id": "MYVVPDv_ESdr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "# # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "# ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]"
      ],
      "metadata": {
        "id": "e9iDwiXWsbsT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# attempt to auto detect device\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "# elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "#     device = \"mps\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-o8roesXFYw",
        "outputId": "2f31c3bb-6cea-4194-e125-70642807568f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed(1337)"
      ],
      "metadata": {
        "id": "de97owKzCY6Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch_size = 524288 # to align with gpt2 training batch size in number of tokens\n",
        "B = 8\n",
        "T = 1024\n",
        "assert total_batch_size % (B*T) == 0, \"make sure total_batch_size is a multiple of B*T\"\n",
        "grad_accum_steps = total_batch_size // (B*T)\n",
        "print(f\"total_batch_size = {total_batch_size}, grad_accum_steps = {grad_accum_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aUODjuWZbpk",
        "outputId": "e54498de-7462-41dd-81bc-e89f921ef4dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_batch_size = 524288, grad_accum_steps = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoaderLite(B = B, T = T)\n",
        "x, y = train_loader.next_batch()\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BQsk3iWDmSV",
        "outputId": "1ac1b38b-088c-49c9-8969-14ca173d4478"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 1024]), torch.Size([8, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision('high') # is not working in T4\n",
        "model_args = dict(vocab_size=50304)\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf) # next number for power of 2\n",
        "model.to(device)\n",
        "model = torch.compile(model) # does not work collab T4"
      ],
      "metadata": {
        "id": "02icu_90QZkP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_lr = max_lr * 0.1\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(decay_ratio * math.pi))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "w3yCl_CcQbPG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logits, loss = model(x, y)\n",
        "# print(loss)\n",
        "# expected loss - -ln(1/505257) = 10.82\n",
        "# AdamW is a bugfix of Adam so to say\n",
        "# optimizer\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "best_loss = 1e9\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    # determine and set learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # with torch.autocast(device_type = device, dtype=torch.bfloat16): # does not work collab T4\n",
        "        with torch.autocast(device_type = device, dtype=torch.float16): # this would need gradient scaling\n",
        "            logits, loss = model(x, y)\n",
        "        loss = loss / grad_accum_steps # loss normalizer\n",
        "        loss_accum += loss.detach()\n",
        "        # loss.backward()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "\n",
        "    # optimizer.step()\n",
        "    scale = scaler.get_scale()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = (t1 - t0)\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if step % log_interval == 0 or step == max_steps-1 or loss_accum.item() < 0.099999:\n",
        "        print(f\"step {step} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "    if step % save_interval == 0 or step == max_steps-1 or loss_accum.item() < 0.099999:\n",
        "        if loss_accum.item() < best_loss:\n",
        "            best_loss = loss_accum.item()\n",
        "            if step > 0:\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': step,\n",
        "                    'best_loss': best_loss,\n",
        "                    }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    if loss_accum.item() < 0.099999:\n",
        "        print(\"Stopping training as reached target loss\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_9-CDKLz-Zl1",
        "outputId": "d68abc13-65a2-4338-f574-6483a8b337c4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0 | loss: 10.983610 | lr: 1.2000e-05 | norm: 9.1490 | dt: 81766.22ms | tok/sec: 6412.04\n",
            "step 50 | loss: 5.600409 | lr: 6.0000e-04 | norm: 0.6819 | dt: 28532.17ms | tok/sec: 18375.33\n",
            "step 100 | loss: 4.467509 | lr: 5.9632e-04 | norm: 0.8411 | dt: 28969.64ms | tok/sec: 18097.84\n",
            "saving checkpoint to saved_model\n",
            "step 150 | loss: 3.736810 | lr: 5.8537e-04 | norm: 1.0279 | dt: 29231.67ms | tok/sec: 17935.62\n",
            "step 200 | loss: 2.950556 | lr: 5.6746e-04 | norm: 2.2319 | dt: 29064.69ms | tok/sec: 18038.65\n",
            "saving checkpoint to saved_model\n",
            "step 250 | loss: 2.031224 | lr: 5.4307e-04 | norm: 3.1140 | dt: 29074.49ms | tok/sec: 18032.58\n",
            "step 300 | loss: 1.301913 | lr: 5.1287e-04 | norm: 4.9952 | dt: 29089.38ms | tok/sec: 18023.35\n",
            "saving checkpoint to saved_model\n",
            "step 350 | loss: 0.543757 | lr: 4.7768e-04 | norm: 3.6370 | dt: 29081.34ms | tok/sec: 18028.33\n",
            "step 400 | loss: 0.214095 | lr: 4.3846e-04 | norm: 2.5734 | dt: 29041.91ms | tok/sec: 18052.80\n",
            "saving checkpoint to saved_model\n",
            "step 424 | loss: 0.096154 | lr: 4.1851e-04 | norm: 1.0759 | dt: 28909.97ms | tok/sec: 18135.19\n",
            "saving checkpoint to saved_model\n",
            "Stopping training as reached target loss\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "print(torch.cuda.list_gpu_processes())\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rbTOYj9t-sXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cf5f84c-9af4-4d93-969c-a3e470d70bee"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU:0\n",
            "process       2630 uses    11842.000 MB GPU memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Generations"
      ],
      "metadata": {
        "id": "O6v4rZ99HVOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30\n",
        "num_return_sequences = 5"
      ],
      "metadata": {
        "id": "kePJ2JPiHW_3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype = torch.long) # (8,)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5,8)\n",
        "x = tokens.to(device)"
      ],
      "metadata": {
        "id": "5Caqp4YOJYQe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate\n",
        "x = model.generate(x, max_new_tokens=max_length)"
      ],
      "metadata": {
        "id": "y6nTHCYVJf7q"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN3LRtLAJwtK",
        "outputId": "c25ac010-b4d2-4aba-c5d2-0c4fc3baa4a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, what consosawatedwell,--pt:\n",
            "Can you work! embroughDid star is one the\n",
            "> Hello, I'm a language model,Royal\n",
            "To think't, take myyre;\n",
            "Had you sought it were mark your wrought malicious,\n",
            "\n",
            "> Hello, I'm a language model, what\n",
            "b parting to the boy. say William, now might known the\n",
            "\n",
            "MARCILAND:\n",
            "\n",
            "> Hello, I'm a language model,ThinkShicoke that\n",
            "Pl Mark minere\n",
            "Have yeISROM O Boling contrens\n",
            "You\n",
            "> Hello, I'm a language model, peace\n",
            "Sweet honest! Thus is smile the speak the or keepThat evade us behold, has I shall.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to hugging face model hub"
      ],
      "metadata": {
        "id": "Cj5ZwEpdG6kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('to_upload', exist_ok=True)"
      ],
      "metadata": {
        "id": "aD7EfJiIQXlb"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model_gpt2.py to_upload\n",
        "!cp -r saved_model to_upload"
      ],
      "metadata": {
        "id": "h76jWzsfHAvD"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(\n",
        "    folder_path=\"./to_upload\",\n",
        "    repo_id=\"sayanbanerjee32/nanogpt2_test\",\n",
        "    repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "5e0ad1c8ef2f4a01b819caa19e001f5c",
            "7c917c376903419f9bff0545b9fd4634",
            "498ccbaf880d4ef6b24a2e035718b320",
            "42fcae08154f4b3998a97d0d11f791a1",
            "42af15fb5d1649ccb80c0ace7de3a027",
            "8307072a74394924ac8c4fa306e3d3af",
            "27ba76910dc549438dc48be30b6fae67",
            "08b34f61482c440ebc0f5ff376249284",
            "6bbdddbf83ff48628f45dac89a85da4a",
            "8b2a63244394477daab48887f6e11297",
            "77f7e31c535a4aa58aacb5133f347b63"
          ]
        },
        "id": "6OAn-nHCHGE9",
        "outputId": "506d1df6-5220-4d43-aa95-87241d5ea1a5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ckpt.pt:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e0ad1c8ef2f4a01b819caa19e001f5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/sayanbanerjee32/nanogpt2_test/commit/569af069ed8cc00766769e74caf8e84b06fd0941', commit_message='Upload folder using huggingface_hub', commit_description='', oid='569af069ed8cc00766769e74caf8e84b06fd0941', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nxNzLHpuxkuW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}