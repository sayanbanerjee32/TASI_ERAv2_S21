{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPIyCn1WmfNoJ9dXsJqbWIB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "974fea332abd4429879d3835846cae44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f0435a0a3ad64dd59b061b389d79284f",
              "IPY_MODEL_de549a76fa2e413d8b4acfce1e80848c",
              "IPY_MODEL_6171f935d5984fce84b85971f05fb759"
            ],
            "layout": "IPY_MODEL_9e919a10e0954cdd899c5e976eb3aa6e"
          }
        },
        "f0435a0a3ad64dd59b061b389d79284f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_30baedb585d14a39b55c963211ef1ef2",
            "placeholder": "​",
            "style": "IPY_MODEL_55e3ff9b8a814f65b823456f6acd2aad",
            "value": "ckpt.pt: 100%"
          }
        },
        "de549a76fa2e413d8b4acfce1e80848c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1d91c29366e246929d4c47b11b894690",
            "max": 1544198298,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b20f815a1e846319b821d5b5c2dea66",
            "value": 1544198298
          }
        },
        "6171f935d5984fce84b85971f05fb759": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d8630f2310b4e88b53d11c3e1633779",
            "placeholder": "​",
            "style": "IPY_MODEL_a572f0b70d414fab81458926a0130b9c",
            "value": " 1.54G/1.54G [00:37&lt;00:00, 47.8MB/s]"
          }
        },
        "9e919a10e0954cdd899c5e976eb3aa6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30baedb585d14a39b55c963211ef1ef2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55e3ff9b8a814f65b823456f6acd2aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1d91c29366e246929d4c47b11b894690": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b20f815a1e846319b821d5b5c2dea66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d8630f2310b4e88b53d11c3e1633779": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a572f0b70d414fab81458926a0130b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sayanbanerjee32/TASI_ERAv2_S21/blob/main/gpt2_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tiktoken\n",
        "!pip install -Uq pynvml"
      ],
      "metadata": {
        "id": "bzpHvONH-2g3"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assignment repo\n",
        "!git clone https://github.com/sayanbanerjee32/TASI_ERAv2_S21.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvO9-EU2GXo0",
        "outputId": "0ee7ce29-7ef0-43a5-a640-dfe7c32e4540"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TASI_ERAv2_S21'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Counting objects: 100% (14/14), done.\u001b[K\n",
            "remote: Compressing objects: 100% (12/12), done.\u001b[K\n",
            "remote: Total 14 (delta 2), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (14/14), 8.57 KiB | 8.57 MiB/s, done.\n",
            "Resolving deltas: 100% (2/2), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## move python files to outside\n",
        "!mv TASI_ERAv2_S21/*.py ."
      ],
      "metadata": {
        "id": "gHEo9KKoGczf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from model_gpt2 import GPTConfig, GPT\n",
        "from data_loader_lite import DataLoaderLite"
      ],
      "metadata": {
        "id": "VXdyZHDrsTy2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "from google.colab import userdata\n",
        "userdata.get('HF_TOKEN')\n",
        "api = HfApi()"
      ],
      "metadata": {
        "id": "GEiP8kqAHKrw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training on tiny shakespeare"
      ],
      "metadata": {
        "id": "6C0WmV7i-JYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzT8AD8v8y-e",
        "outputId": "a7e44a54-b71a-43ee-b019-e4905d0d1421"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-27 11:17:35--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2024-06-27 11:17:36 (30.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### hypre params\n",
        "max_lr = 6e-4\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "warmup_steps = 50\n",
        "max_steps = 1000\n",
        "\n",
        "# save / log config\n",
        "out_dir = 'saved_model'\n",
        "save_interval = 100\n",
        "log_interval = 50"
      ],
      "metadata": {
        "id": "MYVVPDv_ESdr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "# # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler\n",
        "# ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]"
      ],
      "metadata": {
        "id": "e9iDwiXWsbsT"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# attempt to auto detect device\n",
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "# elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
        "#     device = \"mps\"\n",
        "\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-o8roesXFYw",
        "outputId": "a19c0367-b195-40c4-d9ce-4389f56e1e63"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "if device == \"cuda\":\n",
        "    torch.cuda.manual_seed(1337)"
      ],
      "metadata": {
        "id": "de97owKzCY6Q"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_batch_size = 524288 # to align with gpt2 training batch size in number of tokens\n",
        "B = 8\n",
        "T = 1024\n",
        "assert total_batch_size % (B*T) == 0, \"make sure total_batch_size is a multiple of B*T\"\n",
        "grad_accum_steps = total_batch_size // (B*T)\n",
        "print(f\"total_batch_size = {total_batch_size}, grad_accum_steps = {grad_accum_steps}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9aUODjuWZbpk",
        "outputId": "0fec4fe4-e156-4390-c5ac-f32034af9b2f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_batch_size = 524288, grad_accum_steps = 64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoaderLite(B = B, T = T)\n",
        "x, y = train_loader.next_batch()\n",
        "x.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-BQsk3iWDmSV",
        "outputId": "975c3fbc-9bb2-4c08-dce1-23a7fc1ed9ba"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 338025 tokens\n",
            "1 epoch = 41 batches\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([8, 1024]), torch.Size([8, 1024]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision('high') # is not working in T4\n",
        "model_args = dict(vocab_size=50304)\n",
        "gptconf = GPTConfig(**model_args)\n",
        "model = GPT(gptconf) # next number for power of 2\n",
        "model.to(device)\n",
        "model = torch.compile(model) # does not work collab T4"
      ],
      "metadata": {
        "id": "02icu_90QZkP"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_lr = max_lr * 0.1\n",
        "\n",
        "def get_lr(it):\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(decay_ratio * math.pi))\n",
        "\n",
        "    return min_lr + coeff * (max_lr - min_lr)"
      ],
      "metadata": {
        "id": "w3yCl_CcQbPG"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# logits, loss = model(x, y)\n",
        "# print(loss)\n",
        "# expected loss - -ln(1/505257) = 10.82\n",
        "# AdamW is a bugfix of Adam so to say\n",
        "# optimizer\n",
        "\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, betas=(0.9, 0.95), eps=1e-8)\n",
        "optimizer = model.configure_optimizers(weight_decay, max_lr, (beta1, beta2), device)\n",
        "# initialize a GradScaler. If enabled=False scaler is a no-op\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=True)\n",
        "best_loss = 1e9\n",
        "os.makedirs(out_dir, exist_ok=True)\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    # determine and set learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        x, y = train_loader.next_batch()\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        # with torch.autocast(device_type = device, dtype=torch.bfloat16): # does not work collab T4\n",
        "        with torch.autocast(device_type = device, dtype=torch.float16): # this would need gradient scaling\n",
        "            logits, loss = model(x, y)\n",
        "        loss = loss / grad_accum_steps # loss normalizer\n",
        "        loss_accum += loss.detach()\n",
        "        # loss.backward()\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # gradient clipping\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        norm = torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
        "\n",
        "\n",
        "    # optimizer.step()\n",
        "    scale = scaler.get_scale()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "    dt = (t1 - t0)\n",
        "    tokens_processed = train_loader.B * train_loader.T * grad_accum_steps\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if step % log_interval == 0 or step == max_steps-1 or loss_accum.item() < 0.099999:\n",
        "        print(f\"step {step} | loss: {loss_accum.item():.6f} | lr: {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "\n",
        "    if step % save_interval == 0 or step == max_steps-1 or loss_accum.item() < 0.099999:\n",
        "        if loss_accum.item() < best_loss:\n",
        "            best_loss = loss_accum.item()\n",
        "            if step > 0:\n",
        "                checkpoint = {\n",
        "                    'model': model.state_dict(),\n",
        "                    'optimizer': optimizer.state_dict(),\n",
        "                    'model_args': model_args,\n",
        "                    'iter_num': step,\n",
        "                    'best_loss': best_loss,\n",
        "                    }\n",
        "                print(f\"saving checkpoint to {out_dir}\")\n",
        "                torch.save(checkpoint, os.path.join(out_dir, 'ckpt.pt'))\n",
        "\n",
        "    if loss_accum.item() < 0.099999:\n",
        "        print(\"Stopping training as reached target loss\")\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "id": "_9-CDKLz-Zl1",
        "outputId": "19eccf3b-3d08-4b70-d337-938021433161"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
            "using fused AdamW: True\n",
            "step 0 | loss: 10.983610 | lr: 1.2000e-05 | norm: 9.1490 | dt: 81627.05ms | tok/sec: 6422.97\n",
            "step 50 | loss: 5.546760 | lr: 6.0000e-04 | norm: 0.4880 | dt: 28678.36ms | tok/sec: 18281.66\n",
            "step 100 | loss: 4.447003 | lr: 5.9632e-04 | norm: 0.5548 | dt: 28951.61ms | tok/sec: 18109.11\n",
            "saving checkpoint to saved_model\n",
            "step 150 | loss: 3.693120 | lr: 5.8537e-04 | norm: 1.0331 | dt: 29042.14ms | tok/sec: 18052.67\n",
            "step 200 | loss: 2.844754 | lr: 5.6746e-04 | norm: 1.9574 | dt: 29165.84ms | tok/sec: 17976.10\n",
            "saving checkpoint to saved_model\n",
            "step 250 | loss: 2.038058 | lr: 5.4307e-04 | norm: 3.8761 | dt: 29208.81ms | tok/sec: 17949.65\n",
            "step 300 | loss: 1.330756 | lr: 5.1287e-04 | norm: 4.8655 | dt: 29289.51ms | tok/sec: 17900.20\n",
            "saving checkpoint to saved_model\n",
            "step 350 | loss: 0.583206 | lr: 4.7768e-04 | norm: 3.7037 | dt: 29098.63ms | tok/sec: 18017.62\n",
            "step 400 | loss: 0.210908 | lr: 4.3846e-04 | norm: 3.5924 | dt: 29070.43ms | tok/sec: 18035.09\n",
            "saving checkpoint to saved_model\n",
            "step 424 | loss: 0.094815 | lr: 4.1851e-04 | norm: 1.2713 | dt: 28933.98ms | tok/sec: 18120.15\n",
            "saving checkpoint to saved_model\n",
            "step 425 | loss: 0.083899 | lr: 4.1767e-04 | norm: 0.9696 | dt: 28203.17ms | tok/sec: 18589.68\n",
            "saving checkpoint to saved_model\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-e544350daf1f>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmicro_step\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_accum_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# with torch.autocast(device_type = device, dtype=torch.bfloat16): # does not work collab T4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "print(torch.cuda.list_gpu_processes())\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "rbTOYj9t-sXW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e22932-cbc5-443c-d431-bbc985efd340"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU:0\n",
            "process       9121 uses    11842.000 MB GPU memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Generations"
      ],
      "metadata": {
        "id": "O6v4rZ99HVOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 30\n",
        "num_return_sequences = 5"
      ],
      "metadata": {
        "id": "kePJ2JPiHW_3"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix\n",
        "import tiktoken\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = enc.encode(\"Hello, I'm a language model,\")\n",
        "tokens = torch.tensor(tokens, dtype = torch.long) # (8,)\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1) #(5,8)\n",
        "x = tokens.to(device)"
      ],
      "metadata": {
        "id": "5Caqp4YOJYQe"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate\n",
        "x = model.generate(x, max_new_tokens=max_length)"
      ],
      "metadata": {
        "id": "y6nTHCYVJf7q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokens = x[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokens)\n",
        "    print(\">\", decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vN3LRtLAJwtK",
        "outputId": "a1063608-2dfa-40ea-e469-b7f94d57ac67"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "> Hello, I'm a language model, villain cons bolts\n",
            "By goodows care the toward the WhatAn pretend ourT:Being nothing:\n",
            " EL\n",
            "> Hello, I'm a language model, Therefore'sFor prosperous jealous:\n",
            "\n",
            "yre!\n",
            "ThisonI tell't that ask. malicious so purpose\n",
            "> Hello, I'm a language model, what the\n",
            " them the gentle myWill Georgevern such the remorse:\n",
            "So graciousp, whereby I know\n",
            "> Hello, I'm a language model,LoveOM\n",
            "But What hurtship wrought you this Apolloail the how that\n",
            "An time feel the Masters us\n",
            "> Hello, I'm a language model, Do thisUCouch more the from to the how good warrant happyThat evadeator! triumph friend and I will\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload to hugging face model hub"
      ],
      "metadata": {
        "id": "Cj5ZwEpdG6kl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs('to_upload', exist_ok=True)"
      ],
      "metadata": {
        "id": "aD7EfJiIQXlb"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp model_gpt2.py to_upload\n",
        "!cp -r saved_model to_upload"
      ],
      "metadata": {
        "id": "h76jWzsfHAvD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api.upload_folder(\n",
        "    folder_path=\"./to_upload\",\n",
        "    repo_id=\"sayanbanerjee32/nanogpt2_test\",\n",
        "    repo_type=\"model\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "974fea332abd4429879d3835846cae44",
            "f0435a0a3ad64dd59b061b389d79284f",
            "de549a76fa2e413d8b4acfce1e80848c",
            "6171f935d5984fce84b85971f05fb759",
            "9e919a10e0954cdd899c5e976eb3aa6e",
            "30baedb585d14a39b55c963211ef1ef2",
            "55e3ff9b8a814f65b823456f6acd2aad",
            "1d91c29366e246929d4c47b11b894690",
            "9b20f815a1e846319b821d5b5c2dea66",
            "3d8630f2310b4e88b53d11c3e1633779",
            "a572f0b70d414fab81458926a0130b9c"
          ]
        },
        "id": "6OAn-nHCHGE9",
        "outputId": "f5c263d0-606f-40e3-c976-8288efb63a2b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "ckpt.pt:   0%|          | 0.00/1.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "974fea332abd4429879d3835846cae44"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/sayanbanerjee32/nanogpt2_test/commit/11c24e832268a5d66ef78d6c1a2451fa69dc1633', commit_message='Upload folder using huggingface_hub', commit_description='', oid='11c24e832268a5d66ef78d6c1a2451fa69dc1633', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}